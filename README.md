# Chaplin-UI ğŸ¬
![chaplin-ui](https://github.com/user-attachments/assets/529823f7-8766-41ed-87d9-c18b178dc1e0)

<div align="center">

**Visual Speech Recognition - Read lips, transcribe speech, all locally**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

*A beautiful, open-source tool that reads your lips in real-time and transcribes silently mouthed speech using local ML models.*

[Quick Start](#-quick-start) â€¢ [Contributing](./CONTRIBUTING.md) â€¢ [Privacy](./PRIVACY.md) â€¢ [Security](./SECURITY.md) â€¢ [Documentation](#-documentation) â€¢ [License](./LICENSE)

</div>

---

## âœ¨ What is Chaplin-UI?

Chaplin-UI is a gentle, privacy-focused tool that reads lips and turns them into text. Simply record yourself speaking (or upload a video), and watch as your words appear on screenâ€”all without making a sound. This project is based on [Chaplin](https://github.com/amanvirparhar/chaplin) by [Amanvir Parhar](https://amanvir.com), with added web interface and UI improvements. The VSR model achieves **19.1% word error rate (WER)** on LRS3. Perfect for:

- ğŸ¤ **Silent communication** - Type without speaking, or transcribe existing videos
- ğŸ”’ **Privacy-first** - Everything runs locally on your machine ([Privacy Policy](./PRIVACY.md))
- ğŸŒ **Web-based** - Works in any modern browserâ€”no installation needed
- ğŸ¨ **Beautiful UI** - Clean, calming design that adapts to your system theme

## ğŸ’™ Who It's For

I built this after a week of laryngitisâ€”when I couldn't speak, I needed a way to communicate. If you've ever wanted to say something without making a sound, Chaplin-UI might help:

- **Public places** â€” Libraries, offices, late-night calls, or anywhere you want to stay quiet
- **Deaf and hard-of-hearing** â€” Mouth words to communicate when sign language isn't shared
- **Medical conditions** â€” ALS, aphonia, cerebral palsy, laryngectomy, Parkinson's, vocal cord paralysis, selective mutism 
- **Temporary voice loss** â€” Laryngitis, recovery from throat surgery, or vocal strain
- **Privacy** â€” Situations where you'd rather not speak aloud but still need to get your words out

*Apple just acquired a silent-speech company ([Q.ai](https://www.theverge.com/news/870353/apple-q-ai-acquisition-silent-speech)) for $2 billionâ€”this space matters, and open-source tools like this keep the technology accessible.*

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+** (check with `python3 --version`)
- **LLM local server** â€“ Ollama or LM Studio (the app finds one you have running):
  - **[Ollama](https://ollama.com)** â€“ `ollama serve` + `ollama pull <model>`
  - **[LM Studio](https://lmstudio.ai/)** â€“ load model, enable Local Server (port 1234)
- **Modern web browser** with camera access

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/loganngarcia/chaplin-ui.git
   cd chaplin-ui
   ```

2. **Set up Python environment:**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Download model files:**
   ```bash
   ./setup.sh
   ```
   This downloads the VSR model from Hugging Face (~500MB).

4. **Start your LLM server** (pick one: Ollama or LM Studio):

   **Option A â€“ Ollama**
   ```bash
   ollama serve        # usually starts automatically
   ollama pull llama3.2   # or mistral, llama2, etc.
   ```

   **Option B â€“ LM Studio**
   - Open LM Studio
   - Load a model (we recommend `zai-org/glm-4.6v-flash`)
   - Go to **Developer** tab â†’ Enable **Local Server** (port 1234)

5. **Run the web app:**
   ```bash
   ./run_web.sh
   ```
   The UI opens in **~1 second** at [http://localhost:8000](http://localhost:8000). The model loads in the background (~30â€“60 sec first time); you can use the interface right awayâ€”buttons enable when ready.

6. **Start transcribing:**
   - **Record live**: Click "Start recording" to capture video from your camera
   - **Upload a video**: Click "Upload video" to transcribe an existing video file
   - Your transcription appears in both raw and corrected formats. (You can change the LLM in settings if needed.)

   That's it! The app handles everything else for you.

## ğŸ“– Documentation

### Project Structure

```
chaplin-ui/
â”œâ”€â”€ chaplin_ui/              # Core shared modules
â”‚   â””â”€â”€ core/                # Shared utilities, models, configs
â”‚       â”œâ”€â”€ models.py        # Pydantic data models
â”‚       â”œâ”€â”€ constants.py     # All configuration constants
â”‚       â”œâ”€â”€ llm_client.py    # LLM API wrapper
â”‚       â”œâ”€â”€ video_processor.py # Video processing utilities
â”‚       â””â”€â”€ ...
â”œâ”€â”€ web/                     # Web app frontend
â”‚   â”œâ”€â”€ index.html          # Main HTML
â”‚   â”œâ”€â”€ style.css           # Styles (Apple HIG)
â”‚   â””â”€â”€ app.js              # Frontend logic
â”œâ”€â”€ web_app.py              # FastAPI backend server
â”œâ”€â”€ chaplin.py              # CLI implementation
â”œâ”€â”€ main.py                 # CLI entry point
â””â”€â”€ pipelines/              # VSR model pipeline
```

### How It Works

Simply put, Chaplin-UI watches how your lips move and turns that into text. Here's what happens behind the scenes:

1. **You provide video** - Either record yourself speaking or upload an existing video file
2. **Face detection** - The app finds and tracks your face in the video
3. **Lip reading** - A trained model watches your lip movements and creates initial text
4. **Text refinement** - An AI language model cleans up the text, adds punctuation, and fixes any mistakes
5. **You see results** - Both the raw transcription and the polished version appear on screen

You can copy the corrected text with one click, or review the raw output to see what the lip-reading model detected.

### LLM Providers: Ollama vs LM Studio

Chaplin-UI supports two local LLM backends. Both use OpenAI-compatible APIs:

| Provider   | Default URL              | Default Model | Setup |
|-----------|---------------------------|---------------|-------|
| **Ollama** | `http://localhost:11434/v1` | `llama3.2`    | `ollama serve` then `ollama pull <model>` |
| **LM Studio** | `http://localhost:1234/v1` | `local`       | Load model, enable Local Server in Developer tab |

- **Web app**: Select provider in the "LLM Provider" dropdown and optionally override the model name.
- **CLI**: Use `llm_provider=ollama` or `llm_provider=lmstudio`, or run with `--config-name ollama` for Ollama defaults.

### Key Components

- **`chaplin_ui/core/`** - Shared code used by CLI and Web interfaces
- **`web_app.py`** - FastAPI server handling video uploads and processing
- **`chaplin.py`** - CLI version with keyboard typing
- **`pipelines/`** - VSR model inference pipeline

## ğŸ› ï¸ Development

### Running Locally

**Web App:**
```bash
source .venv/bin/activate
python web_app.py
```

**CLI:**
```bash
source .venv/bin/activate
python main.py config_filename=./configs/LRS3_V_WER19.1.ini detector=mediapipe
# With Ollama:
python main.py --config-name ollama
# Or: python main.py llm_provider=ollama llm_model=mistral
```

### Code Style

We follow Python best practices:
- **Type hints** on all functions
- **Docstrings** (Google style) for all public functions
- **Logging** instead of print statements
- **Constants** centralized in `chaplin_ui/core/constants.py`

### Testing

```bash
# Test imports
python -c "from chaplin_ui.core import *; print('âœ“ All imports work')"

# Test web app
python web_app.py &
curl http://localhost:8000/api/health
```

## ğŸ¤ Contributing

We love contributions! Whether it's:
- ğŸ› Bug fixes
- âœ¨ New features
- ğŸ“ Documentation improvements
- ğŸ¨ UI/UX enhancements
- ğŸ”§ Code refactoring

See our [Contributing Guide](./CONTRIBUTING.md) for details on:
- How to set up your development environment
- Code style guidelines
- How to submit pull requests
- Where to ask questions

**First time contributing?** Check out our [good first issues](https://github.com/loganngarcia/chaplin-ui/labels/good%20first%20issue)!

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](./LICENSE) for details.

## ğŸ™ Acknowledgments

### Original Creator

Chaplin-UI is based on [Chaplin](https://github.com/amanvirparhar/chaplin) by **[Amanvir Parhar](https://amanvir.com)**. We're grateful for the original work that made this project possible!

### Additional Credits

- **VSR Model**: Based on [Auto-AVSR](https://github.com/mpc001/auto_avsr) by mpc001 (19.1% WER on LRS3)
- **Dataset**: [Lip Reading Sentences 3](https://mmai.io/datasets/lip_reading/)
- **LLM**: Uses Ollama or LM Studio for local text correction (both OpenAI-compatible)

## ğŸ’¬ Community

- ğŸ› **Found a bug?** [Open an issue](https://github.com/loganngarcia/chaplin-ui/issues)
- ğŸ’¡ **Have an idea?** [Start a discussion](https://github.com/loganngarcia/chaplin-ui/discussions)
- ğŸ“§ **Questions?** Check our [FAQ](./CONTRIBUTING.md#faq) or open a discussion

---

<div align="center">

**Made with â¤ï¸ by the open source community**

[â­ Star us on GitHub](https://github.com/loganngarcia/chaplin-ui) â€¢ [ğŸ“– Read the docs](#-documentation) â€¢ [ğŸ¤ Contribute](./CONTRIBUTING.md)

</div>
