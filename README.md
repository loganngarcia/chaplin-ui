# Chaplin-UI ğŸ¬

<div align="center">

**Visual Speech Recognition - Read lips, transcribe speech, all locally**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

*A beautiful, open-source tool that reads your lips in real-time and transcribes silently mouthed speech using local ML models.*

[Quick Start](#-quick-start) â€¢ [Contributing](./CONTRIBUTING.md) â€¢ [Privacy](./PRIVACY.md) â€¢ [Security](./SECURITY.md) â€¢ [Documentation](#-documentation) â€¢ [License](./LICENSE.md)

</div>

---

## âœ¨ What is Chaplin-UI?

Chaplin-UI is a visual speech recognition (VSR) application that can transcribe speech just by watching your lips move. This project is based on [Chaplin](https://github.com/amanvirparhar/chaplin) by [Amanvir Parhar](https://amanvir.com), with added web interface and UI improvements. Perfect for:

- ğŸ¤ **Silent communication** - Type without speaking
- ğŸ”’ **Privacy-first** - Everything runs locally on your machine ([Privacy Policy](./PRIVACY.md))
- ğŸŒ **Web-based** - Works in any modern browser
- ğŸ¨ **Beautiful UI** - Apple HIG-inspired design

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+** (check with `python3 --version`)
- **LM Studio** ([download here](https://lmstudio.ai/)) - For LLM text correction
- **Modern web browser** with camera access

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/loganngarcia/chaplin-ui.git
   cd chaplin-ui
   ```

2. **Set up Python environment:**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Download model files:**
   ```bash
   ./setup.sh
   ```
   This downloads the VSR model from Hugging Face (~500MB).

4. **Start LM Studio:**
   - Open LM Studio
   - Load a model (we recommend `zai-org/glm-4.6v-flash`)
   - Go to **Developer** tab â†’ Enable **Local Server** (port 1234)

5. **Run the web app:**
   ```bash
   ./run_web.sh
   ```
   Then open [http://localhost:8000](http://localhost:8000) in your browser!

## ğŸ“– Documentation

### Project Structure

```
chaplin-ui/
â”œâ”€â”€ chaplin_ui/              # Core shared modules
â”‚   â””â”€â”€ core/                # Shared utilities, models, configs
â”‚       â”œâ”€â”€ models.py        # Pydantic data models
â”‚       â”œâ”€â”€ constants.py     # All configuration constants
â”‚       â”œâ”€â”€ llm_client.py    # LLM API wrapper
â”‚       â”œâ”€â”€ video_processor.py # Video processing utilities
â”‚       â””â”€â”€ ...
â”œâ”€â”€ web/                     # Web app frontend
â”‚   â”œâ”€â”€ index.html          # Main HTML
â”‚   â”œâ”€â”€ style.css           # Styles (Apple HIG)
â”‚   â””â”€â”€ app.js              # Frontend logic
â”œâ”€â”€ web_app.py              # FastAPI backend server
â”œâ”€â”€ chaplin.py              # CLI implementation
â”œâ”€â”€ main.py                 # CLI entry point
â””â”€â”€ pipelines/              # VSR model pipeline
```

### How It Works

1. **Video Capture**: Camera records video frames (or upload existing video)
2. **Face Detection**: MediaPipe detects and tracks your face
3. **VSR Inference**: LRS3 model processes lip movements â†’ raw text (ALL CAPS)
4. **LLM Correction**: LM Studio corrects grammar, adds punctuation, formats text
5. **Display**: Shows both raw and corrected transcription

### Key Components

- **`chaplin_ui/core/`** - Shared code used by CLI and Web interfaces
- **`web_app.py`** - FastAPI server handling video uploads and processing
- **`chaplin.py`** - CLI version with keyboard typing
- **`pipelines/`** - VSR model inference pipeline

## ğŸ› ï¸ Development

### Running Locally

**Web App:**
```bash
source .venv/bin/activate
python web_app.py
```

**CLI:**
```bash
source .venv/bin/activate
python main.py config_filename=./configs/LRS3_V_WER19.1.ini detector=mediapipe
```

### Code Style

We follow Python best practices:
- **Type hints** on all functions
- **Docstrings** (Google style) for all public functions
- **Logging** instead of print statements
- **Constants** centralized in `chaplin_ui/core/constants.py`

### Testing

```bash
# Test imports
python -c "from chaplin_ui.core import *; print('âœ“ All imports work')"

# Test web app
python web_app.py &
curl http://localhost:8000/api/health
```

## ğŸ¤ Contributing

We love contributions! Whether it's:
- ğŸ› Bug fixes
- âœ¨ New features
- ğŸ“ Documentation improvements
- ğŸ¨ UI/UX enhancements
- ğŸ”§ Code refactoring

See our [Contributing Guide](./CONTRIBUTING.md) for details on:
- How to set up your development environment
- Code style guidelines
- How to submit pull requests
- Where to ask questions

**First time contributing?** Check out our [good first issues](https://github.com/loganngarcia/chaplin-ui/labels/good%20first%20issue)!

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE.md](./LICENSE.md) for details.

## ğŸ™ Acknowledgments

### Original Creator

Chaplin-UI is based on [Chaplin](https://github.com/amanvirparhar/chaplin) by **[Amanvir Parhar](https://amanvir.com)**. We're grateful for the original work that made this project possible!

### Additional Credits

- **VSR Model**: Based on [Auto-AVSR](https://github.com/mpc001/auto_avsr) by mpc001
- **Dataset**: [Lip Reading Sentences 3](https://mmai.io/datasets/lip_reading/)
- **LLM**: Uses LM Studio for local text correction

## ğŸ’¬ Community

- ğŸ› **Found a bug?** [Open an issue](https://github.com/loganngarcia/chaplin-ui/issues)
- ğŸ’¡ **Have an idea?** [Start a discussion](https://github.com/loganngarcia/chaplin-ui/discussions)
- ğŸ“§ **Questions?** Check our [FAQ](./CONTRIBUTING.md#faq) or open a discussion

---

<div align="center">

**Made with â¤ï¸ by the open source community**

[â­ Star us on GitHub](https://github.com/loganngarcia/chaplin-ui) â€¢ [ğŸ“– Read the docs](#-documentation) â€¢ [ğŸ¤ Contribute](./CONTRIBUTING.md)

</div>
